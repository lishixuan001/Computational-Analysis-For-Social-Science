{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping Dictionary Features To All Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Definition] Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import csv\n",
    "import zipfile\n",
    "\n",
    "import threading\n",
    "from multiprocessing import Process, Manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Method] Display Fuction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get name of an object\n",
    "\"\"\"\n",
    "def namestr(obj, namespace):\n",
    "    return [name for name in namespace if namespace[name] is obj]\n",
    "\n",
    "\"\"\"\n",
    "Display with format\n",
    "\"\"\"\n",
    "def display(items, func=None, limit=None):\n",
    "    # Print Variable Name\n",
    "    print(namestr(items, globals()))\n",
    "    # Print Content\n",
    "    count = 0\n",
    "    for item in items:\n",
    "        # Consider Limit\n",
    "        if limit is not None and count >= limit:\n",
    "            return\n",
    "        # Consider Exerted Function\n",
    "        if func:\n",
    "            item = func(item)\n",
    "        # Print Each Item\n",
    "        print(\"     {0}\".format(item))\n",
    "        count += 1\n",
    "\n",
    "### Test ###\n",
    "# test_dict = {\"A\": [1, 2, 3], \"B\": [4, 5, 6]}\n",
    "# display(test_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Method] Display Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_progress(progress, total, lbar_prefix = '', rbar_prefix=''):\n",
    "    percent = round(progress / float(total) * 100, 2)\n",
    "    buf = \"{0}|{1}| {2}{3}/{4} {5}%\\r\".format(lbar_prefix, ('#' * round(percent)).ljust(100, '-'),\n",
    "        rbar_prefix, progress, total, percent)\n",
    "    sys.stdout.write(buf)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "def report_progress_done():\n",
    "    sys.stdout.write('\\n')\n",
    "\n",
    "### TEST ###\n",
    "# total = 100\n",
    "# report_progress(0, total)\n",
    "# for progress in range(1, total + 1):\n",
    "#     time.sleep(0.1)\n",
    "#     report_progress(progress, total)\n",
    "# report_progress_done()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Definition] Define Paths\n",
    "   * Dictionaries: ./Dictionaries [Culture; Demographics; Relational]\n",
    "   * Articles: ../All_Articles [Part 001-098]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "     ('Culture', './Dictionaries/Culture.csv')\n",
      "     ('Demographic', './Dictionaries/Demographic.csv')\n",
      "     ('Relational', './Dictionaries/Relational.csv')\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Paths for Dictionaries\n",
    "\"\"\"\n",
    "dictionary_root = \"./Dictionaries\"\n",
    "dictionary_path = {}\n",
    "dictionary_name_list = [\n",
    "    \"Culture\",\n",
    "    \"Demographic\",\n",
    "    \"Relational\",\n",
    "]\n",
    "\n",
    "for dictionary_name in dictionary_name_list:\n",
    "    dictionary_path[dictionary_name] = join(dictionary_root, dictionary_name + \".csv\")\n",
    "\n",
    "\"\"\"\n",
    "Paths for Articles\n",
    "\"\"\"\n",
    "articles_root = \"../All_Articles\"\n",
    "\n",
    "### TEST ###\n",
    "display(dictionary_path.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Method] Article Zip File Validation By Filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Assert the filename in format \"receipt-id-752441-part-XXX.zip\"\n",
    "where XXX stands for article set number\n",
    "\"\"\"\n",
    "def valid_zip(filename):\n",
    "    return re.match(\"^receipt-id-752441-part-.+.zip$\", filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Application] Article Zip File Validation By Filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['articles_zip_file_list']\n",
      "     receipt-id-752441-part-001.zip\n",
      "     receipt-id-752441-part-002.zip\n",
      "     receipt-id-752441-part-003.zip\n",
      "     receipt-id-752441-part-004.zip\n",
      "     receipt-id-752441-part-005.zip\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Get filtering results\n",
    "\"\"\"\n",
    "articles_zip_file_list = sorted([filename for filename in os.listdir(articles_root) if valid_zip(filename)])\n",
    "display(articles_zip_file_list, limit=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Method] Read Dictionaries' Content\n",
    "    * DataFrame: [Subject; N-Gram; Words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary_dataframe():\n",
    "    \"\"\"\n",
    "    Data\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    # Iterate Through All Dictionaries\n",
    "    for subject_path_pair in dictionary_path.items():\n",
    "        # (Subject, Path) -> ('Culture', './Culture.csv')\n",
    "        subject, path = subject_path_pair[0], subject_path_pair[1]\n",
    "        # Iterate Through All Words In The Dictionary\n",
    "        # Load The .CSV File\n",
    "        with open(path, encoding='ISO-8859-1') as csv_file:\n",
    "            # Define A Line In Data -> [subject, n-gram, words]\n",
    "            dataline = []\n",
    "            # We Do Not Split In Case When There're Multiple Words In A Row\n",
    "            # Since We Store Words As One String In DataFrame\n",
    "            rows = csv.reader(csv_file)\n",
    "            for row in rows:\n",
    "                n_number = len(row)\n",
    "                if n_number <= 0:\n",
    "                    continue\n",
    "                words = row[0].strip()\n",
    "                dataline = [subject, n_number, words]\n",
    "                data.append(dataline)\n",
    "\n",
    "    \"\"\"\n",
    "    Columns\n",
    "    \"\"\"\n",
    "    columns = [\"Subject\", \"N-Gram\", \"Words\"]\n",
    "\n",
    "    \"\"\"\n",
    "    Index\n",
    "    \"\"\"\n",
    "    index = list(range(len(data)))\n",
    "\n",
    "    \"\"\"\n",
    "    DataFrame\n",
    "    \"\"\"\n",
    "    dataframe = pd.DataFrame(data, columns=columns, index=index)\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Application] Read Dictionaries' Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>N-Gram</th>\n",
       "      <th>Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Culture</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Culture</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Culture</td>\n",
       "      <td>1</td>\n",
       "      <td>appropriate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Culture</td>\n",
       "      <td>1</td>\n",
       "      <td>avoidance inspection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Culture</td>\n",
       "      <td>1</td>\n",
       "      <td>bureaucratization</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Subject  N-Gram                 Words\n",
       "0  Culture       1             ambiguity\n",
       "1  Culture       1             ambiguous\n",
       "2  Culture       1           appropriate\n",
       "3  Culture       1  avoidance inspection\n",
       "4  Culture       1     bureaucratization"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrame_dictionary = create_dictionary_dataframe()\n",
    "\n",
    "### TEST ###\n",
    "dataFrame_dictionary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Method] Parse Article Set ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Define function getting article set ID by the zip-file-name\n",
    "Pattern: receipt-id-752441-part-XXX.zip\n",
    "\"\"\"\n",
    "def parse_article_set_id(filename):\n",
    "    id_number_lst = re.findall(\"receipt-id-752441-part-(.+).zip\", filename)\n",
    "    if len(id_number_lst) == 1:\n",
    "        return id_number_lst[0]\n",
    "    print(\"Parse_ID Error: Filename does not match pattern. \")\n",
    "    return None\n",
    "\n",
    "### TEST ###\n",
    "# print(parse_article_set_id(\"receipt-id-752441-part-000.zip\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Method] Parse Article ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Define function getting article ID by the file-name\n",
    "Pattern: journal-article-10.2307_00000000-ngram1.txt\n",
    "\"\"\"\n",
    "def parse_article_id(filename):\n",
    "    id_number_lst = re.findall(\"journal-article-(.+)-ngram.+\", filename)\n",
    "    if len(id_number_lst) == 1:\n",
    "        return id_number_lst[0]\n",
    "    print(\"Parse_ID Error: Filename does not match pattern. \")\n",
    "    return None\n",
    "\n",
    "### TEST ###\n",
    "# print(parse_article_set_id(\"journal-article-10.2307_00000000-ngram1.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Method] Filter Filenames For Test Files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For every file in ngram1/ folder, check the filename validity, \n",
    "extract the article ID, then search if same ID exist in ngram2/3 folders\n",
    "- Expected filename format: journal-article-10.2307_00000000-ngram1.txt\n",
    "@return: {article_id : [T/F, T/F, T/F]}\n",
    "\"\"\"\n",
    "def filter_by_filename(files_list):\n",
    "    filtered_list = {}\n",
    "    for filename in files_list:\n",
    "        assert isinstance(filename, str)\n",
    "        if filename.startswith(\"metadata\"):\n",
    "            continue\n",
    "        # Get n_number\n",
    "        n_number = int(re.findall(\"^ngram(.)/\", filename)[0])\n",
    "        # Check if the filename starts with \"journal-article\"\n",
    "        filename = filename[len(\"ngram\" + str(n_number) + \"/\"):]\n",
    "        if not filename.startswith(\"journal-article\"):\n",
    "            continue\n",
    "        # Get article id\n",
    "        article_id = parse_article_id(filename)\n",
    "        if article_id in filtered_list.keys():\n",
    "            filtered_list[article_id][n_number - 1] = True\n",
    "        else:   \n",
    "            # Initialize existence\n",
    "            existence = [False] * 3\n",
    "            existence[n_number - 1] = True\n",
    "            filtered_list[article_id] = existence\n",
    "            \n",
    "    return filtered_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Method] Check Word Validity For Test Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions checking word attributes (single-letter, starts/ends with numebr)\n",
    "def is_single_letter(word):\n",
    "    assert isinstance(word, str)\n",
    "    return len(word) <= 1\n",
    "\n",
    "def starts_with_number(word):\n",
    "    assert isinstance(word, str)\n",
    "    try:\n",
    "        return word[0].isdigit()\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def ends_with_number(word):\n",
    "    assert isinstance(word, str)\n",
    "    try:\n",
    "        return word[len(word) - 1].isdigit()\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Summary of check functions\n",
    "check_funcs = [\n",
    "    is_single_letter, \n",
    "    starts_with_number, \n",
    "    ends_with_number,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Method] Read N-Gram File And Return Freq List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Read the file without unzipping.\n",
    "@return: freq_list containing all words (all-n-gram) with corresponding freq\n",
    "\"\"\"\n",
    "def get_freq_list(n_number, article_id, zip_file):\n",
    "    \n",
    "    # Read Without Unzipping\n",
    "    ngram_type = \"ngram\" + str(n_number)\n",
    "\n",
    "    # Expected Path: ngram1/journal-article-10.2307_3110425-ngram1.txt\n",
    "    article_path = ngram_type + \"/\"  + \"journal-article-\" + article_id + \"-\" + ngram_type + \".txt\"\n",
    "    try:\n",
    "        article_open = zip_file.open(article_path, mode=\"r\")\n",
    "    except IOError:\n",
    "        print(\"Error opening file {0}\".format(articles_path))\n",
    "        exit(0)\n",
    "\n",
    "    # Initiate freq_list -> [[words0, freq0], [words1, freq1]]\n",
    "    freq_list = []\n",
    "\n",
    "    # Read By Lines\n",
    "    for line in article_open:\n",
    "        line = line.decode(\"utf-8\")\n",
    "\n",
    "        # pair -> \"[\"word1\", \"word2\", \"word3\", \"5\"]\n",
    "        pair = line.strip().split()\n",
    "        assert len(pair) >= 2\n",
    "\n",
    "        # Separate word/freq\n",
    "        words, freq = pair[:-1], pair[-1]\n",
    "        assert freq.isdigit()\n",
    "        \n",
    "        # Words -> \"word1 word2 word3\"\n",
    "        words = \" \".join(words)\n",
    "\n",
    "        # Append new pair to freq_list\n",
    "        freq_list.append([words, freq])\n",
    "\n",
    "    # Close reading file\n",
    "    article_open.close()\n",
    "    \n",
    "    return freq_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Method] Perform Mapping Process And Return Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Mapping words in the freq_list to the dictionaries and get the match rates\n",
    "@return: [Culture_Rate, Demographic_Rate, Relational_Rate]\n",
    "\"\"\"\n",
    "def get_mapping_rate(freq_list, n_number):\n",
    "    \n",
    "    # Initialize the match_counts => {subject : count} ...\n",
    "    match_counts = {}\n",
    "    for subject in dictionary_name_list:\n",
    "        match_counts[subject] = 0\n",
    "        \n",
    "    # Check Through Every Subject Dictionary\n",
    "    for subject in dictionary_name_list:\n",
    "        selected_dictionary = dataFrame_dictionary[(dataFrame_dictionary['Subject'] == subject) & \n",
    "                                      (dataFrame_dictionary['N-Gram'] == n_number)]\n",
    "        count_num = 0\n",
    "        # Iterate Through Each Word In freq_list \n",
    "        for words_freq_pair in freq_list:\n",
    "            words, freq = words_freq_pair\n",
    "        \n",
    "            if selected_dictionary['Words'].str.contains(words).any():\n",
    "                count_num += 1\n",
    "            \n",
    "        match_counts[subject] = count_num\n",
    "    \n",
    "    match_rates = [\n",
    "        match_counts[\"Culture\"] / len(freq_list),\n",
    "        match_counts[\"Demographic\"] / len(freq_list),\n",
    "        match_counts[\"Relational\"] / len(freq_list),\n",
    "    ]\n",
    "\n",
    "    return match_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Articles in a Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_article(zip_file, article_info, data_set_id, total_progress, data):\n",
    "    \n",
    "    article_id, existence = article_info\n",
    "\n",
    "    # Match_Rates => [rates_for_n_1, rates_for_n_2, rates_for_n_3]\n",
    "    match_rates = [None] * len(existence)\n",
    "\n",
    "    # Iterate Through N-Gram Folders\n",
    "    for i in range(len(existence)):\n",
    "        n_number = i + 1\n",
    "\n",
    "        # If the file exists\n",
    "        if existence[i]:\n",
    "            freq_list = get_freq_list(n_number, article_id, zip_file)\n",
    "\n",
    "            if len(freq_list) >= 1:\n",
    "                # [Culture_Rate, Demographic_Rate, Relational_Rate]\n",
    "                match_rates[i] = get_mapping_rate(freq_list, n_number)\n",
    "            else:\n",
    "                match_rates[i] = [None] * 3\n",
    "        else:\n",
    "            match_rates[i] = [None] * 3\n",
    "\n",
    "    # Add data to dataline\n",
    "    dataline = [\n",
    "        data_set_id, \n",
    "        article_id,\n",
    "    ]\n",
    "\n",
    "    # Add N1_Culture; N1_Demographic; N1_Relational; ...\n",
    "    for match_rate_list in match_rates:\n",
    "        for rate in match_rate_list:\n",
    "            dataline.append(rate)\n",
    "\n",
    "    # Add Culture_Rate; Demographic_Rate; Relational_Rate;\n",
    "    subject_rates = [sum([match_rate_list[i] if match_rate_list[i] is not None else 0 for match_rate_list in match_rates]) for i in range(len(dictionary_name_list))]\n",
    "    dataline.extend(subject_rates)\n",
    "\n",
    "    # Add Classification\n",
    "    dataline.append(dictionary_name_list[subject_rates.index(max(subject_rates))])\n",
    "    \n",
    "    # Track Progress\n",
    "    report_progress(len(data), total_progress)\n",
    "    \n",
    "    # Append to data\n",
    "    data.append(dataline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Method] Map Each File's Content To The Dictionary To Calculate The Rate\n",
    "    * DataFrame [Set_ID; File_ID; N1_Culture; N1_Demographic; N1_Relational; N2_Culture; N2_Demographic; N2_Relational; N3_Culture; N3_Demographic; N3_Relational; Culture_Rate; Demographic_Rate; Relational_Rate; Classification;]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "def create_mapping_dataframe():\n",
    "    \n",
    "    \"\"\"\n",
    "    Data\n",
    "    \"\"\"\n",
    "    global data\n",
    "    \n",
    "    # Iterate Through All Article Sets\n",
    "    for article_zip_file_name in articles_zip_file_list:\n",
    "        \n",
    "        # Extract Data Set ID\n",
    "        data_set_id = parse_article_set_id(article_zip_file_name)\n",
    "        \n",
    "        # Path Format => \"../All_Articles/receipt-id-752441-part-000.zip\"\n",
    "        article_zip_file_path = join(articles_root, article_zip_file_name)\n",
    "        \n",
    "        # Read The Zip File Without Unzipping\n",
    "        zip_file = zipfile.ZipFile(article_zip_file_path)\n",
    "        file_name_list = zip_file.namelist()\n",
    "        \n",
    "        # Filter by filename\n",
    "        filtered_file_list = filter_by_filename(file_name_list)\n",
    "        \n",
    "        # Count Progress\n",
    "        total_progress = len(filtered_file_list)\n",
    "        report_progress(0, total_progress)\n",
    "        \n",
    "        # Catch Error\n",
    "        try:\n",
    "            threads = []\n",
    "            \n",
    "            for article_info in filtered_file_list.items():\n",
    "                    \n",
    "                t = threading.Thread(target=process_article, args=(zip_file, article_info, data_set_id, total_progress, data))\n",
    "                threads.append(t)\n",
    "            \n",
    "            for t in threads:\n",
    "                t.start()\n",
    "                \n",
    "            for t in threads:\n",
    "                t.join()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "                \n",
    "        # Report Progree Done        \n",
    "        report_progress_done()   \n",
    "        \n",
    "        break\n",
    "        \n",
    "    \"\"\"\n",
    "    Columns\n",
    "    \"\"\"\n",
    "    columns = [\"Set_ID\", \"File_ID\", \"N1_Culture\", \"N1_Demographic\", \"N1_Relational\", \n",
    "               \"N2_Culture\", \"N2_Demographic\", \"N2_Relational\", \n",
    "               \"N3_Culture\", \"N3_Demographic\", \"N3_Relational\",\n",
    "               \"Culture_Rate\", \"Demographic_Rate\", \"Relational_Rate\", \"Classification\"]\n",
    "\n",
    "    \"\"\"\n",
    "    Index\n",
    "    \"\"\"\n",
    "    index = list(range(len(data)))\n",
    "\n",
    "    \"\"\"\n",
    "    DataFrame\n",
    "    \"\"\"\n",
    "    dataframe = pd.DataFrame(data, columns=columns, index=index)\n",
    "        \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Application] Map Each File's Content To The Dictionary To Calculate The Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|----------------------------------------------------------------------------------------------------| 3/36267 0.01%\r"
     ]
    }
   ],
   "source": [
    "dataFrame_mapping = create_mapping_dataframe()\n",
    "\n",
    "### TEST ###\n",
    "dataFrame_mapping.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Backup] Pool Parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from multiprocessing import Pool, Manager\n",
    "\n",
    "# Create a data\n",
    "# manager = Manager()\n",
    "# data = manager.list()\n",
    "data = []\n",
    "\n",
    "# Best Working Efficiency is 1:1\n",
    "thread_count = multiprocessing.cpu_count() # = Pool().__processes\n",
    "\n",
    "def create_mapping_dataframe():\n",
    "    \n",
    "    \"\"\"\n",
    "    Data\n",
    "    \"\"\"\n",
    "    global data\n",
    "    \n",
    "    # Iterate Through All Article Sets\n",
    "    for article_zip_file_name in articles_zip_file_list:\n",
    "        \n",
    "        # Extract Data Set ID\n",
    "        data_set_id = parse_article_set_id(article_zip_file_name)\n",
    "        \n",
    "        # Path Format => \"../All_Articles/receipt-id-752441-part-000.zip\"\n",
    "        article_zip_file_path = join(articles_root, article_zip_file_name)\n",
    "        \n",
    "        # Read The Zip File Without Unzipping\n",
    "        zip_file = zipfile.ZipFile(article_zip_file_path)\n",
    "        file_name_list = zip_file.namelist()\n",
    "        \n",
    "        # Filter by filename\n",
    "        filtered_file_list = filter_by_filename(file_name_list)\n",
    "        \n",
    "        # Count Progress\n",
    "        total_progress = len(filtered_file_list)\n",
    "        report_progress(0, total_progress)\n",
    "        \n",
    "        # Catch Error\n",
    "        try:\n",
    "            \n",
    "            pool = Pool(processes=thread_count)\n",
    "            \n",
    "            # TEST LIMIT\n",
    "            limit = 0\n",
    "            \n",
    "            for article_info in filtered_file_list.items():\n",
    "                \n",
    "                if limit >= 10:\n",
    "                    break\n",
    "                limit += 1\n",
    "                pool.apply_async(process_article, args=(zip_file, article_info, data_set_id, total_progress, data))  \n",
    "\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "                \n",
    "        # Report Progree Done        \n",
    "        report_progress_done()   \n",
    "        \n",
    "        break\n",
    "        \n",
    "    \"\"\"\n",
    "    Columns\n",
    "    \"\"\"\n",
    "    columns = [\"Set_ID\", \"File_ID\", \"N1_Culture\", \"N1_Demographic\", \"N1_Relational\", \n",
    "               \"N2_Culture\", \"N2_Demographic\", \"N2_Relational\", \n",
    "               \"N3_Culture\", \"N3_Demographic\", \"N3_Relational\",\n",
    "               \"Culture_Rate\", \"Demographic_Rate\", \"Relational_Rate\", \"Classification\"]\n",
    "\n",
    "    \"\"\"\n",
    "    Index\n",
    "    \"\"\"\n",
    "    index = list(range(len(data)))\n",
    "\n",
    "    \"\"\"\n",
    "    DataFrame\n",
    "    \"\"\"\n",
    "    dataframe = pd.DataFrame(list(data), columns=columns, index=index)\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
