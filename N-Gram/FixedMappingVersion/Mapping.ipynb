{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping Dictionary Features To All Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Definition] Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import csv\n",
    "import zipfile\n",
    "\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Method] Display Fuction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get name of an object\n",
    "\"\"\"\n",
    "def namestr(obj, namespace):\n",
    "    return [name for name in namespace if namespace[name] is obj]\n",
    "\n",
    "\"\"\"\n",
    "Display with format\n",
    "\"\"\"\n",
    "def display(items, func=None, limit=None):\n",
    "    # Print Variable Name\n",
    "    print(namestr(items, globals()))\n",
    "    # Print Content\n",
    "    count = 0\n",
    "    for item in items:\n",
    "        # Consider Limit\n",
    "        if limit is not None and count >= limit:\n",
    "            return\n",
    "        # Consider Exerted Function\n",
    "        if func:\n",
    "            item = func(item)\n",
    "        # Print Each Item\n",
    "        print(\"     {0}\".format(item))\n",
    "        count += 1\n",
    "\n",
    "### Test ###\n",
    "# test_dict = {\"A\": [1, 2, 3], \"B\": [4, 5, 6]}\n",
    "# display(test_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Method] Display Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_progress(progress, total, lbar_prefix = '', rbar_prefix=''):\n",
    "    percent = round(progress / float(total) * 100, 2)\n",
    "    buf = \"{0}|{1}| {2}{3}/{4} {5}% \".format(lbar_prefix, ('#' * round(percent)).ljust(100, '-'),\n",
    "        rbar_prefix, progress, total, percent)\n",
    "    sys.stdout.write(buf)\n",
    "    sys.stdout.write('\\r')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "def report_progress_done():\n",
    "    sys.stdout.write('\\n')\n",
    "\n",
    "### TEST ###\n",
    "# total = 100\n",
    "# report_progress(0, total)\n",
    "# for progress in range(1, total + 1):\n",
    "#     time.sleep(0.1)\n",
    "#     report_progress(progress, total)\n",
    "# report_progress_done()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Definition] Define Paths\n",
    "   * Dictionaries: ./Dictionaries [Culture; Demographics; Relational]\n",
    "   * Articles: ../All_Articles [Part 001-098]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "     ('Culture', './Dictionaries/Culture.csv')\n",
      "     ('Demographic', './Dictionaries/Demographic.csv')\n",
      "     ('Relational', './Dictionaries/Relational.csv')\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Paths for Dictionaries\n",
    "\"\"\"\n",
    "dictionary_root = \"./Dictionaries\"\n",
    "dictionary_path = {}\n",
    "dictionary_name_list = [\n",
    "    \"Culture\",\n",
    "    \"Demographic\",\n",
    "    \"Relational\",\n",
    "]\n",
    "\n",
    "for dictionary_name in dictionary_name_list:\n",
    "    dictionary_path[dictionary_name] = join(dictionary_root, dictionary_name + \".csv\")\n",
    "\n",
    "\"\"\"\n",
    "Paths for Articles\n",
    "\"\"\"\n",
    "articles_root = \"../All_Articles\"\n",
    "\n",
    "\"\"\"\n",
    "Paths for Database\n",
    "\"\"\"\n",
    "db_root = \"./\"\n",
    "db_name = \"my_result.db\"\n",
    "\n",
    "### TEST ###\n",
    "display(dictionary_path.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Method] Article Zip File Validation By Filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Assert the filename in format \"receipt-id-752441-part-XXX.zip\"\n",
    "where XXX stands for article set number\n",
    "\"\"\"\n",
    "def valid_zip(filename):\n",
    "    return re.match(\"^receipt-id-752441-part-.+.zip$\", filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Application] Article Zip File Validation By Filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['articles_zip_file_list']\n",
      "     receipt-id-752441-part-001.zip\n",
      "     receipt-id-752441-part-002.zip\n",
      "     receipt-id-752441-part-003.zip\n",
      "     receipt-id-752441-part-004.zip\n",
      "     receipt-id-752441-part-005.zip\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Get filtering results\n",
    "\"\"\"\n",
    "articles_zip_file_list = sorted([filename for filename in os.listdir(articles_root) if valid_zip(filename)])\n",
    "display(articles_zip_file_list, limit=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Method] Read Dictionaries' Content\n",
    "    * DataFrame: [Subject; N-Gram; Words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary_dataframe():\n",
    "    \"\"\"\n",
    "    Data\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    # Iterate Through All Dictionaries\n",
    "    for subject_path_pair in dictionary_path.items():\n",
    "        # (Subject, Path) -> ('Culture', './Culture.csv')\n",
    "        subject, path = subject_path_pair[0], subject_path_pair[1]\n",
    "        # Iterate Through All Words In The Dictionary\n",
    "        # Load The .CSV File\n",
    "        with open(path, encoding='ISO-8859-1') as csv_file:\n",
    "            # Define A Line In Data -> [subject, n-gram, words]\n",
    "            dataline = []\n",
    "            # We Do Not Split In Case When There're Multiple Words In A Row\n",
    "            # Since We Store Words As One String In DataFrame\n",
    "            rows = csv.reader(csv_file)\n",
    "            for row in rows:\n",
    "                n_number = len(row)\n",
    "                if n_number <= 0:\n",
    "                    continue\n",
    "                words = row[0].strip()\n",
    "                dataline = [subject, n_number, words]\n",
    "                data.append(dataline)\n",
    "\n",
    "    \"\"\"\n",
    "    Columns\n",
    "    \"\"\"\n",
    "    columns = [\"Subject\", \"N-Gram\", \"Words\"]\n",
    "\n",
    "    \"\"\"\n",
    "    Index\n",
    "    \"\"\"\n",
    "    index = list(range(len(data)))\n",
    "\n",
    "    \"\"\"\n",
    "    DataFrame\n",
    "    \"\"\"\n",
    "    dataframe = pd.DataFrame(data, columns=columns, index=index)\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Application] Read Dictionaries' Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>N-Gram</th>\n",
       "      <th>Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Culture</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Culture</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Culture</td>\n",
       "      <td>1</td>\n",
       "      <td>appropriate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Culture</td>\n",
       "      <td>1</td>\n",
       "      <td>avoidance inspection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Culture</td>\n",
       "      <td>1</td>\n",
       "      <td>bureaucratization</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Subject  N-Gram                 Words\n",
       "0  Culture       1             ambiguity\n",
       "1  Culture       1             ambiguous\n",
       "2  Culture       1           appropriate\n",
       "3  Culture       1  avoidance inspection\n",
       "4  Culture       1     bureaucratization"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrame_dictionary = create_dictionary_dataframe()\n",
    "\n",
    "### TEST ###\n",
    "dataFrame_dictionary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Method] Parse Article Set ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Define function getting article set ID by the zip-file-name\n",
    "Pattern: receipt-id-752441-part-XXX.zip\n",
    "\"\"\"\n",
    "def parse_article_set_id(filename):\n",
    "    id_number_lst = re.findall(\"receipt-id-752441-part-(.+).zip\", filename)\n",
    "    if len(id_number_lst) == 1:\n",
    "        return id_number_lst[0]\n",
    "    print(\"Parse_ID Error: Filename does not match pattern. \")\n",
    "    return None\n",
    "\n",
    "### TEST ###\n",
    "# print(parse_article_set_id(\"receipt-id-752441-part-000.zip\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Method] Parse Article ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Define function getting article ID by the file-name\n",
    "Pattern: journal-article-10.2307_00000000-ngram1.txt\n",
    "\"\"\"\n",
    "def parse_article_id(filename):\n",
    "    id_number_lst = re.findall(\"journal-article-(.+)-ngram.+\", filename)\n",
    "    if len(id_number_lst) == 1:\n",
    "        return id_number_lst[0]\n",
    "    print(\"Parse_ID Error: Filename does not match pattern. \")\n",
    "    return None\n",
    "\n",
    "### TEST ###\n",
    "# print(parse_article_set_id(\"journal-article-10.2307_00000000-ngram1.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Method] Filter Filenames For Test Files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For every file in ngram1/ folder, check the filename validity, \n",
    "extract the article ID, then search if same ID exist in ngram2/3 folders\n",
    "- Expected filename format: journal-article-10.2307_00000000-ngram1.txt\n",
    "@return: {article_id : [T/F, T/F, T/F]}\n",
    "\"\"\"\n",
    "def filter_by_filename(files_list):\n",
    "    filtered_list = {}\n",
    "    for filename in files_list:\n",
    "        assert isinstance(filename, str)\n",
    "        if filename.startswith(\"metadata\"):\n",
    "            continue\n",
    "        # Get n_number\n",
    "        n_number = int(re.findall(\"^ngram(.)/\", filename)[0])\n",
    "        # Check if the filename starts with \"journal-article\"\n",
    "        filename = filename[len(\"ngram\" + str(n_number) + \"/\"):]\n",
    "        if not filename.startswith(\"journal-article\"):\n",
    "            continue\n",
    "        # Get article id\n",
    "        article_id = parse_article_id(filename)\n",
    "        if article_id in filtered_list.keys():\n",
    "            filtered_list[article_id][n_number - 1] = True\n",
    "        else:   \n",
    "            # Initialize existence\n",
    "            existence = [False] * 3\n",
    "            existence[n_number - 1] = True\n",
    "            filtered_list[article_id] = existence\n",
    "            \n",
    "    return filtered_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Method] Check Word Validity For Test Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions checking word attributes (single-letter, starts/ends with numebr)\n",
    "def is_single_letter(word):\n",
    "    assert isinstance(word, str)\n",
    "    return len(word) <= 1\n",
    "\n",
    "def starts_with_number(word):\n",
    "    assert isinstance(word, str)\n",
    "    try:\n",
    "        return word[0].isdigit()\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def ends_with_number(word):\n",
    "    assert isinstance(word, str)\n",
    "    try:\n",
    "        return word[len(word) - 1].isdigit()\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Summary of check functions\n",
    "check_funcs = [\n",
    "    is_single_letter, \n",
    "    starts_with_number, \n",
    "    ends_with_number,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Method] Read N-Gram File And Return Freq List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Read the file without unzipping.\n",
    "@return: freq_list containing all words (all-n-gram) with corresponding freq\n",
    "\"\"\"\n",
    "def get_freq_list(n_number, article_id, zip_file):\n",
    "    \n",
    "    # Read Without Unzipping\n",
    "    ngram_type = \"ngram\" + str(n_number)\n",
    "\n",
    "    # Expected Path: ngram1/journal-article-10.2307_3110425-ngram1.txt\n",
    "    article_path = ngram_type + \"/\"  + \"journal-article-\" + article_id + \"-\" + ngram_type + \".txt\"\n",
    "    try:\n",
    "        article_open = zip_file.open(article_path, mode=\"r\")\n",
    "    except IOError:\n",
    "        print(\"Error opening file {0}\".format(articles_path))\n",
    "        exit(0)\n",
    "\n",
    "    # Initiate freq_list -> [[words0, freq0], [words1, freq1]]\n",
    "    freq_list = []\n",
    "\n",
    "    # Read By Lines\n",
    "    for line in article_open:\n",
    "        line = line.decode(\"utf-8\")\n",
    "\n",
    "        # pair -> \"[\"word1\", \"word2\", \"word3\", \"5\"]\n",
    "        pair = line.strip().split()\n",
    "        assert len(pair) >= 2\n",
    "\n",
    "        # Separate word/freq\n",
    "        words, freq = pair[:-1], pair[-1]\n",
    "        assert freq.isdigit()\n",
    "        \n",
    "        # Words -> \"word1 word2 word3\"\n",
    "        words = \" \".join(words)\n",
    "\n",
    "        # Append new pair to freq_list\n",
    "        freq_list.append([words, freq])\n",
    "\n",
    "    # Close reading file\n",
    "    article_open.close()\n",
    "    \n",
    "    return freq_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Method] Perform Mapping Process And Return Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Mapping words in the freq_list to the dictionaries and get the match rates\n",
    "@return: [Culture_Rate, Demographic_Rate, Relational_Rate]\n",
    "\"\"\"\n",
    "def get_mapping_rate(freq_list):\n",
    "    \n",
    "    # Initialize the match_counts => {subject : count} ...\n",
    "    match_counts = {}\n",
    "    for subject in dictionary_name_list:\n",
    "        match_counts[subject] = 0\n",
    "    \n",
    "    # Iterate Through Each Word In freq_list \n",
    "    for words_freq_pair in freq_list:\n",
    "        words, freq = words_freq_pair\n",
    "        \n",
    "        # Identify n_number\n",
    "        n_number = len(words)\n",
    "        \n",
    "        # Check Through Every Subject Dictionary\n",
    "        for subject in dictionary_name_list:\n",
    "            selected_dictionary = dataFrame_dictionary[(dataFrame_dictionary['Subject'] == subject) & \n",
    "                                          (dataFrame_dictionary['N-Gram'] == n_number)]\n",
    "            if selected_dictionary['Words'].str.contains(words).any():\n",
    "                match_counts[subject] += 1\n",
    "    \n",
    "    match_rates = [\n",
    "        match_counts[\"Culture\"] / len(freq_list),\n",
    "        match_counts[\"Demographic\"] / len(freq_list),\n",
    "        match_counts[\"Relational\"] / len(freq_list),\n",
    "    ]\n",
    "\n",
    "    return match_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Method] Map Each File's Content To The Dictionary To Calculate The Rate\n",
    "    * DataFrame [Set_ID; File_ID; N1_Culture; N1_Demographic; N1_Relational; N2_Culture; N2_Demographic; N2_Relational; N3_Culture; N3_Demographic; N3_Relational; Culture_Rate; Demographic_Rate; Relational_Rate; Classification;]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The function runs the mapping algorithm and stores every result into the database\n",
    "@param break_point: the article_id of the last article the last time we finished \n",
    "\"\"\"\n",
    "def create_mapping_operation(break_point=None):\n",
    "    \n",
    "    # Connect to the database \"map_result.db\"\n",
    "    conn = sqlite3.connect(join(db_root, db_name))\n",
    "    # Create Cursor object so that we can execute SQL commands\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    \"\"\"\n",
    "    Columns\n",
    "    \"\"\"\n",
    "    columns = [\"Set_ID\", \"File_ID\", \n",
    "               \"N1_Culture\", \"N1_Demographic\", \"N1_Relational\", \n",
    "               \"N2_Culture\", \"N2_Demographic\", \"N2_Relational\", \n",
    "               \"N3_Culture\", \"N3_Demographic\", \"N3_Relational\",\n",
    "               \"Culture_Rate\", \"Demographic_Rate\", \"Relational_Rate\", \"Classification\"]\n",
    "    \n",
    "    \"\"\"\n",
    "    Data\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    # Iterate Through All Article Sets\n",
    "    for article_zip_file_name in articles_zip_file_list:\n",
    "        \n",
    "        # Extract Data Set ID\n",
    "        data_set_id = parse_article_set_id(article_zip_file_name)\n",
    "        \n",
    "        # Path Format => \"../All_Articles/receipt-id-752441-part-000.zip\"\n",
    "        article_zip_file_path = join(articles_root, article_zip_file_name)\n",
    "        \n",
    "        # Read The Zip File Without Unzipping\n",
    "        zip_file = zipfile.ZipFile(article_zip_file_path)\n",
    "        file_name_list = zip_file.namelist()\n",
    "        \n",
    "        # Filter by filename\n",
    "        filtered_file_list = filter_by_filename(file_name_list)\n",
    "        \n",
    "        # Count Progress\n",
    "        total_progress = len(filtered_file_list)\n",
    "        report_progress(0, total_progress)\n",
    "        count = 0\n",
    "        \n",
    "        # Catch Error\n",
    "        try:\n",
    "            # Start Working (continue from the last break point)\n",
    "            start_index = True if break_point is None else False\n",
    "            \n",
    "            # Iterate Each Article Through the filtered_file_list\n",
    "            for article_info in filtered_file_list.items():\n",
    "                article_id, existence = article_info\n",
    "                \n",
    "                # Track Progress\n",
    "                count += 1\n",
    "                report_progress(count, total_progress)\n",
    "                \n",
    "                # Set a starting article_id\n",
    "                if not start_index:\n",
    "                    if article_id == break_point:\n",
    "                        start_index = True\n",
    "                    continue\n",
    "\n",
    "                # Match_Rates => [rates_for_n_1, rates_for_n_2, rates_for_n_3]\n",
    "                match_rates = [None] * len(existence)\n",
    "\n",
    "                # Iterate Through N-Gram Folders\n",
    "                for i in range(len(existence)):\n",
    "                    n_number = i + 1\n",
    "\n",
    "                    # If the file exists\n",
    "                    if existence[i]:\n",
    "                        freq_list = get_freq_list(n_number, article_id, zip_file)\n",
    "\n",
    "                        if len(freq_list) >= 1:\n",
    "                            # [Culture_Rate, Demographic_Rate, Relational_Rate]\n",
    "                            match_rates[i] = get_mapping_rate(freq_list)\n",
    "                        else:\n",
    "                            match_rates[i] = [None] * 3\n",
    "                    else:\n",
    "                        match_rates[i] = [None] * 3\n",
    "\n",
    "                # Add data to dataline\n",
    "                dataline = [\n",
    "                    data_set_id, \n",
    "                    article_id,\n",
    "                ]\n",
    "\n",
    "                # Add N1_Culture; N1_Demographic; N1_Relational; ...\n",
    "                for match_rate_list in match_rates:\n",
    "                    for rate in match_rate_list:\n",
    "                        dataline.append(rate)\n",
    "\n",
    "                # Add Culture_Rate; Demographic_Rate; Relational_Rate;\n",
    "                subject_rates = [sum([match_rate_list[i] if match_rate_list[i] is not None else 0 for match_rate_list in match_rates]) for i in range(len(dictionary_name_list))]\n",
    "                dataline.extend(subject_rates)\n",
    "\n",
    "                # Add Classification\n",
    "                dataline.append(dictionary_name_list[subject_rates.index(max(subject_rates))])\n",
    "                \n",
    "                # Write the dataline into the database\n",
    "                assert len(dataline) == len(columns)\n",
    "                insert_value = \"insert into map_result values \" \\\n",
    "                    \"('{set_id}', '{file_id}', \" \\\n",
    "                    \"{n1_culture}, {n1_demographic}, {n1_relational}, \" \\\n",
    "                    \"{n2_culture}, {n2_demographic}, {n2_relational}, \" \\\n",
    "                    \"{n3_culture}, {n3_demographic}, {n3_relational}, \" \\\n",
    "                    \"{culture_rate}, {demographic_rate}, {relational_rate}, '{classification}')\".format(set_id=dataline[0], file_id=dataline[1], \n",
    "                                                                                                       n1_culture=dataline[2], n1_demographic=dataline[3], n1_relational=dataline[4], \n",
    "                                                                                                       n2_culture=dataline[5], n2_demographic=dataline[6], n2_relational=dataline[7],\n",
    "                                                                                                       n3_culture=dataline[8], n3_demographic=dataline[9], n3_relational=dataline[10],\n",
    "                                                                                                       culture_rate=dataline[11], demographic_rate=dataline[12], relational_rate=dataline[13], classification=dataline[14])\n",
    "                cur.execute(insert_value)\n",
    "                conn.commit()\n",
    "                \n",
    "                # # Append value to data\n",
    "                # data.append(dataline)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            cur.close()\n",
    "            conn.close()\n",
    "            pass\n",
    "                \n",
    "        # Report Progree Done        \n",
    "        report_progress_done()   \n",
    "        \n",
    "        break\n",
    "    \n",
    "    # Close the database and cursor\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "#     \"\"\"\n",
    "#     Index\n",
    "#     \"\"\"\n",
    "#     index = list(range(len(data)))\n",
    "\n",
    "#     \"\"\"\n",
    "#     DataFrame\n",
    "#     \"\"\"\n",
    "#     dataframe = pd.DataFrame(data, columns=columns, index=index)\n",
    "    \n",
    "#     return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Application] Map Each File's Content To The Dictionary To Calculate The Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|----------------------------------------------------------------------------------------------------| 62/36267 0.17% \r"
     ]
    }
   ],
   "source": [
    "create_mapping_operation(break_point=\"10.2307_40405624\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataFrame_mapping = create_mapping_dataframe()\n",
    "\n",
    "### TEST ###\n",
    "# dataFrame_mapping.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run This If Empty Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the database \"map_result.db\"\n",
    "conn = sqlite3.connect(join(db_root, db_name))\n",
    "# Create Cursor object so that we can execute SQL commands\n",
    "cur = conn.cursor()\n",
    "# Create table\n",
    "create_table = \"create table books (title text, author text, lang text) \"\n",
    "create_table = 'create table if not exists map_result ' \\\n",
    "    '(set_id text, file_id text, ' \\\n",
    "    'n1_culture real, n1_demographic real, n1_relational real, ' \\\n",
    "    'n2_culture real, n2_demographic real, n2_relational real, ' \\\n",
    "    'n3_culture real, n3_demographic real, n3_relational real, ' \\\n",
    "    'culture_rate real, demographic_rate real, relational_rate real, classification text) '\n",
    "cur.execute(create_table)\n",
    "\n",
    "# # Insert value\n",
    "# insert_value = \"insert into map_result values \" \\\n",
    "#     \"('set_id_sample', 'file_id_sample', \" \\\n",
    "#     \"0.01, 0.01, 0.01, \" \\\n",
    "#     \"0.02, 0.02, 0.02, \" \\\n",
    "#     \"0.03, 0.03, 0.03, \" \\\n",
    "#     \"0.99, 0.99, 0.99, 'culture')\"\n",
    "# cur.execute(insert_value)\n",
    "\n",
    "# Commit the changes and close\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "     001\n",
      "     10.2307_40405624\n",
      "     0.014072847682119206\n",
      "     0.0024834437086092716\n",
      "     0.015728476821192054\n",
      "     0.0\n",
      "     0.0\n",
      "     0.0\n",
      "     0.0\n",
      "     0.0\n",
      "     0.0\n",
      "     0.014072847682119206\n",
      "     0.0024834437086092716\n",
      "     0.015728476821192054\n",
      "     Relational\n"
     ]
    }
   ],
   "source": [
    "# Connect to the database \"map_result.db\"\n",
    "conn = sqlite3.connect(join(db_root, db_name))\n",
    "# Create Cursor object so that we can execute SQL commands\n",
    "cur = conn.cursor()\n",
    "cur.execute('select * from map_result')\n",
    "display(cur.fetchall()[-1])\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
