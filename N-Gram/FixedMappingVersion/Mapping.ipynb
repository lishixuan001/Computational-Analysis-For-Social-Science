{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping Dictionary Features To All Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Definition] Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import csv\n",
    "import zipfile\n",
    "\n",
    "import sqlite3\n",
    "import pickle\n",
    "\n",
    "import logging\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Method] Logger Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logger(name):\n",
    "    now = datetime.datetime.now().strftime('%Y-%m-%d-%H:%M:%S')\n",
    "    formatter = logging.Formatter(fmt='%(asctime)s %(levelname)-8s %(message)s',\n",
    "                                  datefmt='%Y-%m-%d %H:%M:%S')\n",
    "    handler = logging.FileHandler('log/{}.log'.format(now), mode='w')\n",
    "    handler.setFormatter(formatter)\n",
    "\n",
    "    screen_handler = logging.StreamHandler(stream=sys.stdout)\n",
    "    screen_handler.setFormatter(formatter)\n",
    "\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    logger.addHandler(handler)\n",
    "    logger.addHandler(screen_handler)\n",
    "    \n",
    "    return logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Method] Display Fuction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get name of an object\n",
    "\"\"\"\n",
    "def namestr(obj, namespace):\n",
    "    return [name for name in namespace if namespace[name] is obj]\n",
    "\n",
    "\"\"\"\n",
    "Display with format\n",
    "\"\"\"\n",
    "def display(items, func=None, limit=None):\n",
    "    # Print Variable Name\n",
    "    print(namestr(items, globals()))\n",
    "    # Print Content\n",
    "    count = 0\n",
    "    for item in items:\n",
    "        # Consider Limit\n",
    "        if limit is not None and count >= limit:\n",
    "            return\n",
    "        # Consider Exerted Function\n",
    "        if func:\n",
    "            item = func(item)\n",
    "        # Print Each Item\n",
    "        print(\"     {0}\".format(item))\n",
    "        count += 1\n",
    "\n",
    "### Test ###\n",
    "# test_dict = {\"A\": [1, 2, 3], \"B\": [4, 5, 6]}\n",
    "# display(test_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Method] Display Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_progress(progress, total, avg_time='', lbar_prefix = '', rbar_prefix=''):\n",
    "    percent = round(progress / float(total) * 100, 2)\n",
    "    buf = \"{0}|{1}| {2}{3}/{4} {5} [{6}]% \".format(lbar_prefix, ('#' * round(percent)).ljust(100, '-'),\n",
    "        rbar_prefix, progress, total, percent, avg_time)\n",
    "    sys.stdout.write(buf)\n",
    "    sys.stdout.write('\\r')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "\n",
    "## TEST ###\n",
    "# total = 100\n",
    "# report_progress(0, total)\n",
    "# for progress in range(1, total + 1):\n",
    "#     time.sleep(0.01)\n",
    "#     report_progress(progress, total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Definition] Define Paths\n",
    "   * Dictionaries: ./Dictionaries [Culture; Demographics; Relational]\n",
    "   * Articles: ../All_Articles [Part 001-098]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Paths for Dictionaries\n",
    "\"\"\"\n",
    "dictionary_root = \"./Dictionaries\"\n",
    "dictionary_path = {}\n",
    "dictionary_name_list = [\n",
    "    \"Culture\",\n",
    "    \"Demographic\",\n",
    "    \"Relational\",\n",
    "]\n",
    "\n",
    "for dictionary_name in dictionary_name_list:\n",
    "    dictionary_path[dictionary_name] = join(dictionary_root, dictionary_name + \".csv\")\n",
    "\n",
    "\"\"\"\n",
    "Paths for Articles\n",
    "\"\"\"\n",
    "zip_articles_root = \"../Soc_MGT_OB_1980_2018\"\n",
    "extracted_articles_root = \"../ExtractedZipFiles\"\n",
    "demo_files_root = \"../ExtractedZipFiles/demo\"\n",
    "\n",
    "\"\"\"\n",
    "Paths for Database\n",
    "\"\"\"\n",
    "db_root = \"./\"\n",
    "db_name = \"map_result.db\"\n",
    "\n",
    "\"\"\"\n",
    "File ID Iteration\n",
    "\"\"\"\n",
    "standard_folder = \"metadata\"\n",
    "ngram_types = [\"ngram1\", \"ngram2\", \"ngram3\"]\n",
    "\n",
    "\"\"\"\n",
    "Load file ID differences\n",
    "\"\"\"\n",
    "with open(\"./df_diffs_sum.gz\", \"rb\") as df_diffs_file:\n",
    "    df_diffs_sum = pickle.load(df_diffs_file)\n",
    "\n",
    "\n",
    "### TEST ###\n",
    "# display(dictionary_path.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Method] Article Zip File Validation By Filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "receipt-id-989431-part-001\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Assert the filename in format \"receipt-id-989431-part-001\"\n",
    "where XXX stands for article set number\n",
    "\"\"\"\n",
    "def valid_file_set(filename):\n",
    "    return re.match(\"^receipt-id-989431-part-(.+)$\", filename)\n",
    "\n",
    "### TEST ###\n",
    "# print(valid_file_set(\"receipt-id-989431-part-001\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Application] Article Zip File Validation By Filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['articles_file_set_list']\n",
      "     receipt-id-989431-part-014\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Get filtering results\n",
    "\"\"\"\n",
    "articles_file_set_list = sorted([filename for filename in os.listdir(extracted_articles_root) if valid_file_set(filename)])\n",
    "display(articles_file_set_list, limit=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Method] Parse Article Set ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "014\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Define function getting article set ID by the zip-file-name\n",
    "Pattern: receipt-id-989431-part-XXX.zip\n",
    "\"\"\"\n",
    "def parse_article_set_id(filename):\n",
    "    id_number_lst = re.findall(\"receipt-id-989431-part-(.+)$\", filename)\n",
    "    if len(id_number_lst) == 1:\n",
    "        return id_number_lst[0]\n",
    "    print(\"Parse_ID Error: Filename does not match pattern. \")\n",
    "    return None\n",
    "\n",
    "### TEST ###\n",
    "# print(parse_article_set_id(\"receipt-id-989431-part-014\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Method] Parse Article Number ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.1086_210007\n",
      "10.1086_210007\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Define function getting article ID by the file-name\n",
    "Pattern: journal-article-10.1086_210007-ngram1.txt\n",
    "\"\"\"\n",
    "def parse_article_id(filename):\n",
    "    id_number_lst = re.findall(\"journal-article-(.+)-+\", filename)\n",
    "    if len(id_number_lst) == 1:\n",
    "        return id_number_lst[0]\n",
    "    id_number_lst = re.findall(\"journal-article-(.+)\\.+\", filename)\n",
    "    if len(id_number_lst) == 1:\n",
    "        return id_number_lst[0]\n",
    "    print(\"Parse_ID Error: Filename does not match pattern. \")\n",
    "    return None\n",
    "\n",
    "### TEST ###\n",
    "# print(parse_article_id(\"journal-article-10.1086_210007-ngram1.txt\"))\n",
    "# print(parse_article_id(\"journal-article-10.1086_210007.xml\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Method] Read Dictionaries' Content\n",
    "    * DataFrame: [Subject; N-Gram; Words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary_dataframe():\n",
    "    \"\"\"\n",
    "    Data\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    # Iterate Through All Dictionaries\n",
    "    for subject_path_pair in dictionary_path.items():\n",
    "        # (Subject, Path) -> ('Culture', './Culture.csv')\n",
    "        subject, path = subject_path_pair[0], subject_path_pair[1]\n",
    "        # Iterate Through All Words In The Dictionary\n",
    "        # Load The .CSV File\n",
    "        with open(path, encoding='ISO-8859-1') as csv_file:\n",
    "            # Define A Line In Data -> [subject, n-gram, words]\n",
    "            dataline = []\n",
    "            # We Do Not Split In Case When There're Multiple Words In A Row\n",
    "            # Since We Store Words As One String In DataFrame\n",
    "            rows = csv.reader(csv_file)\n",
    "            for row in rows:\n",
    "                n_number = len(row)\n",
    "                if n_number <= 0:\n",
    "                    continue\n",
    "                words = \" \".join(row)\n",
    "                ngram_type = \"ngram{n_number}\".format(n_number=n_number)\n",
    "                dataline = [subject, ngram_type, words]\n",
    "                data.append(dataline)\n",
    "\n",
    "    \"\"\"\n",
    "    Columns\n",
    "    \"\"\"\n",
    "    columns = [\"Subject\", \"N-Gram\", \"Words\"]\n",
    "\n",
    "    \"\"\"\n",
    "    Index\n",
    "    \"\"\"\n",
    "    index = list(range(len(data)))\n",
    "\n",
    "    \"\"\"\n",
    "    DataFrame\n",
    "    \"\"\"\n",
    "    dataframe = pd.DataFrame(data, columns=columns, index=index)\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Application] Read Dictionaries' Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>N-Gram</th>\n",
       "      <th>Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Culture</td>\n",
       "      <td>ngram1</td>\n",
       "      <td>ambiguity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Culture</td>\n",
       "      <td>ngram1</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Culture</td>\n",
       "      <td>ngram1</td>\n",
       "      <td>appropriate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Culture</td>\n",
       "      <td>ngram2</td>\n",
       "      <td>avoidance inspection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Culture</td>\n",
       "      <td>ngram1</td>\n",
       "      <td>bureaucratization</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Subject  N-Gram                 Words\n",
       "0  Culture  ngram1             ambiguity\n",
       "1  Culture  ngram1             ambiguous\n",
       "2  Culture  ngram1           appropriate\n",
       "3  Culture  ngram2  avoidance inspection\n",
       "4  Culture  ngram1     bureaucratization"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrame_dictionary = create_dictionary_dataframe()\n",
    "\n",
    "### TEST ###\n",
    "# dataFrame_dictionary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Method] Filter Filenames For Test Files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For every file in ngram1/ folder, check the filename validity, \n",
    "extract the article ID, then search if same ID exist in ngram2/3 folders\n",
    "- Expected filename format: journal-article-10.2307_00000000-ngram1.txt\n",
    "@return: {article_id : [T/F, T/F, T/F]}\n",
    "\"\"\"\n",
    "def filter_by_filename(files_list):\n",
    "    filtered_list = {}\n",
    "    for filename in files_list:\n",
    "        assert isinstance(filename, str)\n",
    "        if filename.startswith(\"metadata\"):\n",
    "            continue\n",
    "        # Get n_number\n",
    "        n_number = int(re.findall(\"^ngram(.)/\", filename)[0])\n",
    "        # Check if the filename starts with \"journal-article\"\n",
    "        filename = filename[len(\"ngram\" + str(n_number) + \"/\"):]\n",
    "        if not filename.startswith(\"journal-article\"):\n",
    "            continue\n",
    "        # Get article id\n",
    "        article_id = parse_article_id(filename)\n",
    "        if article_id in filtered_list.keys():\n",
    "            filtered_list[article_id][n_number - 1] = True\n",
    "        else:   \n",
    "            # Initialize existence\n",
    "            existence = [False] * 3\n",
    "            existence[n_number - 1] = True\n",
    "            filtered_list[article_id] = existence\n",
    "            \n",
    "    return filtered_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Method] Check Word Validity For Test Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions checking word attributes (single-letter, starts/ends with numebr)\n",
    "def is_single_letter(word):\n",
    "    assert isinstance(word, str)\n",
    "    return len(word) <= 1\n",
    "\n",
    "def starts_with_number(word):\n",
    "    assert isinstance(word, str)\n",
    "    try:\n",
    "        return word[0].isdigit()\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def ends_with_number(word):\n",
    "    assert isinstance(word, str)\n",
    "    try:\n",
    "        return word[len(word) - 1].isdigit()\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Summary of check functions\n",
    "check_funcs = [\n",
    "    is_single_letter, \n",
    "    starts_with_number, \n",
    "    ends_with_number,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Method] Read N-Gram File And Return Freq List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@return: [[\"word1 word2 word3\", 5], [\"word4 word5 word6\", 2], ...]\n",
    "         freq_list containing all words (all-n-gram) with corresponding freq\n",
    "\"\"\"\n",
    "def get_freq_list(file_path):\n",
    "    \n",
    "    with open(file_path, mode=\"r\", encoding=\"utf-8\") as article_open:\n",
    "\n",
    "        # Initiate freq_list -> [[words0, freq0], [words1, freq1]]\n",
    "        freq_list = []\n",
    "\n",
    "        # Read By Lines\n",
    "        for line in article_open:\n",
    "            \n",
    "            # pair -> \"[\"word1\", \"word2\", \"word3\", \"5\"]\n",
    "            pair = line.strip().split()\n",
    "            assert len(pair) >= 2\n",
    "\n",
    "            # Separate word/freq\n",
    "            words, freq = pair[:-1], pair[-1]\n",
    "            assert freq.isdigit()\n",
    "            \n",
    "            check_words = [check_func(word) for word in words for check_func in check_funcs]\n",
    "            \n",
    "            if any(check_words):\n",
    "                continue\n",
    "\n",
    "            # Words -> \"word1 word2 word3\"\n",
    "            words = \" \".join(words)\n",
    "\n",
    "            # Append new pair to freq_list\n",
    "            freq_list.append([words, freq])\n",
    "    \n",
    "    return freq_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Method] Perform Mapping Process And Return Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Mapping words in the freq_list to the dictionaries and get the match rates\n",
    "@return: [Culture_Rate, Demographic_Rate, Relational_Rate]\n",
    "\"\"\"\n",
    "def get_mapping_rate(file_path, ngram_type):\n",
    "    \"\"\"\n",
    "    :param file_path: complete directory path of the file\n",
    "    :param ngram_type: \"ngram1\" /OR/ \"ngram2\" /OR/ \"ngram3\"\n",
    "    :return: Mapping_Rates, Success => [Culture_Rate, Demographic_Rate, Relational_Rate], True\n",
    "                                    => [None, None, None], False\n",
    "    \"\"\"\n",
    "    \n",
    "    # If filepath does not exist, return [None, None, None]\n",
    "    if not isfile(file_path):\n",
    "        return list([None, None, None]), False\n",
    "    \n",
    "    # Get frequent list\n",
    "    freq_list = get_freq_list(file_path)\n",
    "    \n",
    "    # Initialize the match_counts => {\"Culture\" : 0, ...} \n",
    "    match_counts = {}\n",
    "    for subject in dictionary_name_list:\n",
    "        match_counts[subject] = 0\n",
    "    \n",
    "    # Iterate Through Each Word In freq_list \n",
    "    # -> [[\"word1 word2 word3\", freq], ...]\n",
    "    for words_freq_pair in freq_list:\n",
    "        words, freq = words_freq_pair\n",
    "        \n",
    "        # Check Through Every Subject Dictionary\n",
    "        for subject in dictionary_name_list:\n",
    "            selected_dictionary = dataFrame_dictionary[(dataFrame_dictionary['Subject'] == subject) & \n",
    "                                          (dataFrame_dictionary['N-Gram'] == ngram_type)]\n",
    "            if selected_dictionary['Words'].str.contains(words).any():\n",
    "                match_counts[subject] += 1\n",
    "    \n",
    "    match_rates = [\n",
    "        match_counts[\"Culture\"] / len(freq_list),\n",
    "        match_counts[\"Demographic\"] / len(freq_list),\n",
    "        match_counts[\"Relational\"] / len(freq_list),\n",
    "    ]\n",
    "\n",
    "    return match_rates, True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Method] Map Each File's Content To The Dictionary To Calculate The Rate\n",
    "    * DataFrame [Set_ID; File_ID; N1_Culture; N1_Demographic; N1_Relational; N2_Culture; N2_Demographic; N2_Relational; N3_Culture; N3_Demographic; N3_Relational; Culture_Rate; Demographic_Rate; Relational_Rate; Classification;]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manage_file(file_id, file_set_id, file_set_path):\n",
    "\n",
    "    dataline = []\n",
    "    dataline.append(file_set_id)\n",
    "    dataline.append(file_id)\n",
    "\n",
    "    # Iterate through all folders\n",
    "    # -> [\"ngram1\", \"ngram2\", \"ngram3\"]\n",
    "    cdr_rates = list([0, 0, 0])\n",
    "    for ngram_type in ngram_types:\n",
    "        folder_path = join(file_set_path, ngram_type)\n",
    "\n",
    "        file_name = \"journal-article-{file_id}-{ngram_type}.txt\".format(file_id=file_id, \n",
    "                                                                          ngram_type=ngram_type)\n",
    "        file_path = join(folder_path, file_name)\n",
    "\n",
    "        # culture_rate, demographic_rate, relational_rate = [0.15, 0.093, 0.125]\n",
    "        cdr_ngram_rates, success = get_mapping_rate(file_path, ngram_type) \n",
    "        if success:\n",
    "            cdr_rates = [sum(pair) for pair in zip(cdr_rates, cdr_ngram_rates)]\n",
    "        else:\n",
    "            logger.warning(\"[File_Not_Found] => [FileSetID: {file_set_id}], \\\n",
    "            [FileID: {file_id}], [NGramType: {ngram_type}]\".format(file_set_id=file_set_id,\n",
    "                                                                   file_id=file_id,\n",
    "                                                                   ngram_type=ngram_type))\n",
    "\n",
    "        dataline.extend(cdr_ngram_rates)\n",
    "    # Add general prediction probabilities\n",
    "    dataline.extend(cdr_rates)\n",
    "\n",
    "    # Get prediction\n",
    "    prediction = dictionary_name_list[cdr_rates.index(max(cdr_rates))]\n",
    "    dataline.append(prediction)\n",
    "\n",
    "    return dataline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The function runs the mapping algorithm and stores every result into the database\n",
    "@param break_point: the article_id of the last article the last time we finished running the function\n",
    "\"\"\"\n",
    "def create_mapping_operation(break_point=None):\n",
    "    \n",
    "    logger = setup_logger(\"Mapping\")\n",
    "    \n",
    "    logger.info(\"Connecting to DB\")\n",
    "    \n",
    "    # Connect to the database \"map_result.db\"\n",
    "    conn = sqlite3.connect(join(db_root, db_name))\n",
    "    # Create Cursor object so that we can execute SQL commands\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    logger.info(\"Start Iterative Mapping\")\n",
    "\n",
    "    # In case of non-closing database cursor which left db open\n",
    "    try:\n",
    "        # Iterate Through All Article Sets \n",
    "        # -> [\"receipt-id-989431-part-014\", \"receipt-id-989431-part-013\", ...]\n",
    "        for article_file_set_name in articles_file_set_list:\n",
    "            \n",
    "            pool = multiprocessing.Pool(14)\n",
    "\n",
    "            # Extract Data Set ID\n",
    "            # -> \"014\"\n",
    "            file_set_id = parse_article_set_id(article_file_set_name)\n",
    "            file_set_path = join(extracted_articles_root, article_file_set_name)\n",
    "            \n",
    "            logger.info(\"Computing File Set: [{file_set_id}]\".format(file_set_id=file_set_id))\n",
    "\n",
    "            # Get file IDs from \"metadata\" folder\n",
    "            # -> [\"10.1086_210007\", \"10.1086_1038856\", ...]\n",
    "            file_ids = [parse_article_id(filename) for filename in listdir(join(file_set_path, standard_folder))]\n",
    "            \n",
    "            index = 1\n",
    "            if break_point is not None:\n",
    "                index = file_ids.index(break_point)\n",
    "                file_ids = file_ids[index+1 : ]\n",
    "                \n",
    "            logger.info(\"Multi-Processing Files \")\n",
    "            \n",
    "            pool = multiprocessing.Pool(8)\n",
    "            results = [pool.apply_async(manage_file, (file_id, file_set_id, file_set_path)) for file_id in file_ids]\n",
    "            \n",
    "            logger.info(\"Async Running -- To Be Completed\")\n",
    "            \n",
    "            start = time.time()\n",
    "            count, total = index, len(file_ids)\n",
    "            for result in results:\n",
    "                dataline = result.get()\n",
    "\n",
    "                # Write the dataline into the database\n",
    "                insert_value = \"insert into map_result values \" \\\n",
    "                    \"('{set_id}', '{file_id}', \" \\\n",
    "                    \"{n1_culture}, {n1_demographic}, {n1_relational}, \" \\\n",
    "                    \"{n2_culture}, {n2_demographic}, {n2_relational}, \" \\\n",
    "                    \"{n3_culture}, {n3_demographic}, {n3_relational}, \" \\\n",
    "                    \"{culture_rate}, {demographic_rate}, {relational_rate}, '{classification}')\".format(set_id=dataline[0], file_id=dataline[1], \n",
    "                                                                                                       n1_culture=dataline[2], n1_demographic=dataline[3], n1_relational=dataline[4], \n",
    "                                                                                                       n2_culture=dataline[5], n2_demographic=dataline[6], n2_relational=dataline[7],\n",
    "                                                                                                       n3_culture=dataline[8], n3_demographic=dataline[9], n3_relational=dataline[10],\n",
    "                                                                                                       culture_rate=dataline[11], demographic_rate=dataline[12], relational_rate=dataline[13], classification=dataline[14])\n",
    "                cur.execute(insert_value)\n",
    "                conn.commit()\n",
    "                \n",
    "                # Update Progress\n",
    "                count += 1\n",
    "                avg_time = (time.time() - start) / count\n",
    "                report_progress(count, total, avg_time)\n",
    "\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(\"Error Message: {}\\n\".format(e))\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        return\n",
    "        \n",
    "    # Close the database and cursor\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Method] Get Breakpoint Article ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_breakpoint_article_id():\n",
    "    # Connect to the database \"map_result.db\"\n",
    "    conn = sqlite3.connect(join(db_root, db_name))\n",
    "    # Create Cursor object so that we can execute SQL commands\n",
    "    cur = conn.cursor()\n",
    "    # Select all data entries from the table \n",
    "    cur.execute('SELECT * FROM map_result')\n",
    "    # Display all data collected\n",
    "    database_collection = cur.fetchall()\n",
    "    print(\"Total Count: {count}\".format(count=len(database_collection)))\n",
    "    # Close the cursor and the database\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    \n",
    "    if len(database_collection) > 0:\n",
    "        display(database_collection[-1])\n",
    "        breakpoint_id = database_collection[-1][1]\n",
    "        return breakpoint_id\n",
    "    return None\n",
    "        \n",
    "\n",
    "\n",
    "### TEST ###\n",
    "# breakpoint_id = get_breakpoint_article_id()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Application] Map Each File's Content To The Dictionary To Calculate The Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-27 16:51:32 INFO     Connecting to DB\n",
      "2019-02-27 16:51:32 INFO     Connecting to DB\n",
      "2019-02-27 16:51:32 INFO     Connecting to DB\n",
      "2019-02-27 16:51:32 INFO     Connecting to DB\n",
      "2019-02-27 16:51:32 INFO     Connecting to DB\n",
      "2019-02-27 16:51:32 INFO     Connecting to DB\n",
      "2019-02-27 16:51:32 INFO     Connecting to DB\n",
      "2019-02-27 16:51:32 INFO     Connecting to DB\n",
      "2019-02-27 16:51:32 INFO     Start Iterative Mapping\n",
      "2019-02-27 16:51:32 INFO     Start Iterative Mapping\n",
      "2019-02-27 16:51:32 INFO     Start Iterative Mapping\n",
      "2019-02-27 16:51:32 INFO     Start Iterative Mapping\n",
      "2019-02-27 16:51:32 INFO     Start Iterative Mapping\n",
      "2019-02-27 16:51:32 INFO     Start Iterative Mapping\n",
      "2019-02-27 16:51:32 INFO     Start Iterative Mapping\n",
      "2019-02-27 16:51:32 INFO     Start Iterative Mapping\n",
      "2019-02-27 16:51:32 INFO     Computing File Set: [014]\n",
      "2019-02-27 16:51:32 INFO     Computing File Set: [014]\n",
      "2019-02-27 16:51:32 INFO     Computing File Set: [014]\n",
      "2019-02-27 16:51:32 INFO     Computing File Set: [014]\n",
      "2019-02-27 16:51:32 INFO     Computing File Set: [014]\n",
      "2019-02-27 16:51:32 INFO     Computing File Set: [014]\n",
      "2019-02-27 16:51:32 INFO     Computing File Set: [014]\n",
      "2019-02-27 16:51:32 INFO     Computing File Set: [014]\n",
      "2019-02-27 16:51:33 INFO     Multi-Processing Files \n",
      "2019-02-27 16:51:33 INFO     Multi-Processing Files \n",
      "2019-02-27 16:51:33 INFO     Multi-Processing Files \n",
      "2019-02-27 16:51:33 INFO     Multi-Processing Files \n",
      "2019-02-27 16:51:33 INFO     Multi-Processing Files \n",
      "2019-02-27 16:51:33 INFO     Multi-Processing Files \n",
      "2019-02-27 16:51:33 INFO     Multi-Processing Files \n",
      "2019-02-27 16:51:33 INFO     Multi-Processing Files \n",
      "2019-02-27 16:51:38 INFO     Async Running -- To Be Completed\n",
      "2019-02-27 16:51:38 INFO     Async Running -- To Be Completed\n",
      "2019-02-27 16:51:38 INFO     Async Running -- To Be Completed\n",
      "2019-02-27 16:51:38 INFO     Async Running -- To Be Completed\n",
      "2019-02-27 16:51:38 INFO     Async Running -- To Be Completed\n",
      "2019-02-27 16:51:38 INFO     Async Running -- To Be Completed\n",
      "2019-02-27 16:51:38 INFO     Async Running -- To Be Completed\n",
      "2019-02-27 16:51:38 INFO     Async Running -- To Be Completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-378:\n",
      "Process ForkPoolWorker-368:\n",
      "Process ForkPoolWorker-373:\n",
      "Process ForkPoolWorker-372:\n",
      "Process ForkPoolWorker-377:\n",
      "Process ForkPoolWorker-376:\n",
      "Process ForkPoolWorker-367:\n",
      "Process ForkPoolWorker-375:\n",
      "Process ForkPoolWorker-379:\n",
      "Process ForkPoolWorker-374:\n",
      "Process ForkPoolWorker-369:\n",
      "Process ForkPoolWorker-371:\n",
      "Process ForkPoolWorker-370:\n",
      "Process ForkPoolWorker-366:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-ba08e6d023f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcreate_mapping_operation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbreak_point\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-58-fe86af73b3fa>\u001b[0m in \u001b[0;36mcreate_mapping_operation\u001b[0;34m(break_point)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdataline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0;31m# Write the dataline into the database\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-384:\n",
      "Process ForkPoolWorker-381:\n",
      "Process ForkPoolWorker-385:\n",
      "Process ForkPoolWorker-386:\n",
      "Process ForkPoolWorker-382:\n",
      "Process ForkPoolWorker-387:\n",
      "Process ForkPoolWorker-380:\n",
      "Traceback (most recent call last):\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "Traceback (most recent call last):\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "Traceback (most recent call last):\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "Traceback (most recent call last):\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "Traceback (most recent call last):\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "Traceback (most recent call last):\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"<ipython-input-37-44c5ad2e9a68>\", line 18, in manage_file\n",
      "    cdr_ngram_rates, success = get_mapping_rate(file_path, ngram_type)\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-36-ead3459d9c03>\", line 32, in get_mapping_rate\n",
      "    selected_dictionary = dataFrame_dictionary[(dataFrame_dictionary['Subject'] == subject) &\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"<ipython-input-37-44c5ad2e9a68>\", line 18, in manage_file\n",
      "    cdr_ngram_rates, success = get_mapping_rate(file_path, ngram_type)\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"<ipython-input-37-44c5ad2e9a68>\", line 18, in manage_file\n",
      "    cdr_ngram_rates, success = get_mapping_rate(file_path, ngram_type)\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-37-44c5ad2e9a68>\", line 18, in manage_file\n",
      "    cdr_ngram_rates, success = get_mapping_rate(file_path, ngram_type)\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-36-ead3459d9c03>\", line 32, in get_mapping_rate\n",
      "    selected_dictionary = dataFrame_dictionary[(dataFrame_dictionary['Subject'] == subject) &\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"<ipython-input-36-ead3459d9c03>\", line 32, in get_mapping_rate\n",
      "    selected_dictionary = dataFrame_dictionary[(dataFrame_dictionary['Subject'] == subject) &\n",
      "  File \"<ipython-input-37-44c5ad2e9a68>\", line 18, in manage_file\n",
      "    cdr_ngram_rates, success = get_mapping_rate(file_path, ngram_type)\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"<ipython-input-36-ead3459d9c03>\", line 33, in get_mapping_rate\n",
      "    (dataFrame_dictionary['N-Gram'] == ngram_type)]\n",
      "  File \"<ipython-input-37-44c5ad2e9a68>\", line 18, in manage_file\n",
      "    cdr_ngram_rates, success = get_mapping_rate(file_path, ngram_type)\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/site-packages/pandas/core/ops.py\", line 887, in wrapper\n",
      "    res = pd.Series(res, index=self.index, name=self.name, dtype='bool')\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/site-packages/pandas/core/ops.py\", line 887, in wrapper\n",
      "    res = pd.Series(res, index=self.index, name=self.name, dtype='bool')\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/site-packages/pandas/core/ops.py\", line 879, in wrapper\n",
      "    res = na_op(values, other)\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"<ipython-input-37-44c5ad2e9a68>\", line 18, in manage_file\n",
      "    cdr_ngram_rates, success = get_mapping_rate(file_path, ngram_type)\n",
      "  File \"<ipython-input-36-ead3459d9c03>\", line 33, in get_mapping_rate\n",
      "    (dataFrame_dictionary['N-Gram'] == ngram_type)]\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-36-ead3459d9c03>\", line 34, in get_mapping_rate\n",
      "    if selected_dictionary['Words'].str.contains(words).any():\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/site-packages/pandas/core/ops.py\", line 879, in wrapper\n",
      "    res = na_op(values, other)\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/site-packages/pandas/core/ops.py\", line 879, in wrapper\n",
      "    res = na_op(values, other)\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/site-packages/pandas/core/series.py\", line 264, in __init__\n",
      "    raise_cast_failure=True)\n",
      "  File \"<ipython-input-36-ead3459d9c03>\", line 33, in get_mapping_rate\n",
      "    (dataFrame_dictionary['N-Gram'] == ngram_type)]\n",
      "KeyboardInterrupt\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/site-packages/pandas/core/ops.py\", line 783, in na_op\n",
      "    result = _comp_method_OBJECT_ARRAY(op, x, y)\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/site-packages/pandas/core/ops.py\", line 783, in na_op\n",
      "    result = _comp_method_OBJECT_ARRAY(op, x, y)\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/site-packages/pandas/core/strings.py\", line 1567, in contains\n",
      "    regex=regex)\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/site-packages/pandas/core/series.py\", line 266, in __init__\n",
      "    data = SingleBlockManager(data, index, fastpath=True)\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/site-packages/pandas/core/series.py\", line 3246, in _sanitize_array\n",
      "    if getattr(subarr, 'ndim', 0) == 0:\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/site-packages/pandas/core/ops.py\", line 783, in na_op\n",
      "    result = _comp_method_OBJECT_ARRAY(op, x, y)\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/site-packages/pandas/core/ops.py\", line 754, in _comp_method_OBJECT_ARRAY\n",
      "    if isinstance(y, (np.ndarray, ABCSeries, ABCIndex)):\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/site-packages/pandas/core/strings.py\", line 245, in str_contains\n",
      "    if regex:\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/site-packages/pandas/core/ops.py\", line 763, in _comp_method_OBJECT_ARRAY\n",
      "    result = lib.scalar_compare(x, y, op)\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/site-packages/pandas/core/internals.py\", line 4402, in __init__\n",
      "    fastpath=True)\n",
      "KeyboardInterrupt\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/site-packages/pandas/core/dtypes/generic.py\", line 9, in _check\n",
      "    return getattr(inst, attr, '_typ') in comp\n",
      "KeyboardInterrupt\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/site-packages/pandas/core/ops.py\", line 763, in _comp_method_OBJECT_ARRAY\n",
      "    result = lib.scalar_compare(x, y, op)\n",
      "  File \"pandas/_libs/lib.pyx\", line 647, in pandas._libs.lib.scalar_compare\n",
      "KeyboardInterrupt\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/site-packages/pandas/core/frame.py\", line 2133, in __getitem__\n",
      "    return self._getitem_array(key)\n",
      "KeyboardInterrupt\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/site-packages/pandas/core/frame.py\", line 2175, in _getitem_array\n",
      "    return self._take(indexer, axis=0, convert=False)\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 951, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 886, in _find_spec\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/site-packages/pandas/core/generic.py\", line 2150, in _take\n",
      "    verify=True)\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/site-packages/pandas/core/internals.py\", line 4264, in take\n",
      "    axis=axis, allow_dups=True)\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/site-packages/pandas/core/internals.py\", line 4154, in reindex_indexer\n",
      "    return self.__class__(new_blocks, new_axes)\n",
      "  File \"<frozen importlib._bootstrap>\", line 845, in __enter__\n",
      "KeyboardInterrupt\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/site-packages/pandas/core/internals.py\", line 3037, in __init__\n",
      "    self._rebuild_blknos_and_blklocs()\n",
      "  File \"/global/software/sl-7.x86_64/modules/langs/python/3.6/lib/python3.6/site-packages/pandas/core/internals.py\", line 3128, in _rebuild_blknos_and_blklocs\n",
      "    if (new_blknos == -1).any():\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "create_mapping_operation(break_point=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [BackUp] Un-Parallelized Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The function runs the mapping algorithm and stores every result into the database\n",
    "@param break_point: the article_id of the last article the last time we finished running the function\n",
    "\"\"\"\n",
    "def create_mapping_operation(break_point=None):\n",
    "    \n",
    "    logger = setup_logger(\"Mapping\")\n",
    "    \n",
    "    logger.info(\"Connecting to DB\")\n",
    "    \n",
    "    # Connect to the database \"map_result.db\"\n",
    "    conn = sqlite3.connect(join(db_root, db_name))\n",
    "    # Create Cursor object so that we can execute SQL commands\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    logger.info(\"Start Iterative Mapping\")\n",
    "\n",
    "    # In case of non-closing database cursor which left db open\n",
    "    try:\n",
    "        # Iterate Through All Article Sets \n",
    "        # -> [\"receipt-id-989431-part-014\", \"receipt-id-989431-part-013\", ...]\n",
    "        for article_file_set_name in articles_file_set_list:\n",
    "            \n",
    "            pool = multiprocessing.Pool(14)\n",
    "\n",
    "            # Extract Data Set ID\n",
    "            # -> \"014\"\n",
    "            file_set_id = parse_article_set_id(article_file_set_name)\n",
    "            file_set_path = join(extracted_articles_root, article_file_set_name)\n",
    "            \n",
    "            logger.info(\"Computing File Set: [{file_set_id}]\".format(file_set_id=file_set_id))\n",
    "\n",
    "            # Get file IDs from \"metadata\" folder\n",
    "            # -> [\"10.1086_210007\", \"10.1086_1038856\", ...]\n",
    "            file_ids = [parse_article_id(filename) for filename in listdir(join(file_set_path, standard_folder))]\n",
    "\n",
    "            # Iterate through file IDs\n",
    "            count, total = 1, len(file_ids)\n",
    "            continue_run = False\n",
    "            for file_id in file_ids:\n",
    "                \n",
    "                # For breakpoint continuing\n",
    "                if not continue_run and break_point is not None:\n",
    "                    if file_id == break_point:\n",
    "                        continue_run = True\n",
    "                    continue\n",
    "\n",
    "                dataline = []\n",
    "                dataline.append(file_set_id)\n",
    "                dataline.append(file_id)\n",
    "\n",
    "                # Iterate through all folders\n",
    "                # -> [\"ngram1\", \"ngram2\", \"ngram3\"]\n",
    "                cdr_rates = list([0, 0, 0])\n",
    "                for ngram_type in ngram_types:\n",
    "                    folder_path = join(file_set_path, ngram_type)\n",
    "\n",
    "                    file_name = \"journal-article-{file_id}-{ngram_type}.txt\".format(file_id=file_id, \n",
    "                                                                                      ngram_type=ngram_type)\n",
    "                    file_path = join(folder_path, file_name)\n",
    "\n",
    "                    # culture_rate, demographic_rate, relational_rate = [0.15, 0.093, 0.125]\n",
    "                    cdr_ngram_rates, success = get_mapping_rate(file_path, ngram_type) \n",
    "                    if success:\n",
    "                        cdr_rates = [sum(pair) for pair in zip(cdr_rates, cdr_ngram_rates)]\n",
    "                    else:\n",
    "                        logger.warning(\"[File_Not_Found] => [FileSetID: {file_set_id}], \\\n",
    "                        [FileID: {file_id}], [NGramType: {ngram_type}]\".format(file_set_id=file_set_id,\n",
    "                                                                               file_id=file_id,\n",
    "                                                                               ngram_type=ngram_type))\n",
    "\n",
    "                    dataline.extend(cdr_ngram_rates)\n",
    "                # Add general prediction probabilities\n",
    "                dataline.extend(cdr_rates)\n",
    "\n",
    "                # Get prediction\n",
    "                prediction = dictionary_name_list[cdr_rates.index(max(cdr_rates))]\n",
    "                dataline.append(prediction)\n",
    "\n",
    "                # Write the dataline into the database\n",
    "                insert_value = \"insert into map_result values \" \\\n",
    "                    \"('{set_id}', '{file_id}', \" \\\n",
    "                    \"{n1_culture}, {n1_demographic}, {n1_relational}, \" \\\n",
    "                    \"{n2_culture}, {n2_demographic}, {n2_relational}, \" \\\n",
    "                    \"{n3_culture}, {n3_demographic}, {n3_relational}, \" \\\n",
    "                    \"{culture_rate}, {demographic_rate}, {relational_rate}, '{classification}')\".format(set_id=dataline[0], file_id=dataline[1], \n",
    "                                                                                                       n1_culture=dataline[2], n1_demographic=dataline[3], n1_relational=dataline[4], \n",
    "                                                                                                       n2_culture=dataline[5], n2_demographic=dataline[6], n2_relational=dataline[7],\n",
    "                                                                                                       n3_culture=dataline[8], n3_demographic=dataline[9], n3_relational=dataline[10],\n",
    "                                                                                                       culture_rate=dataline[11], demographic_rate=dataline[12], relational_rate=dataline[13], classification=dataline[14])\n",
    "                cur.execute(insert_value)\n",
    "                conn.commit()\n",
    "                \n",
    "                # Update Progress\n",
    "                count += 1\n",
    "                report_progress(count, total)\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(\"Error Message: {}\\n\".format(e))\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        return\n",
    "        \n",
    "    # Close the database and cursor\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    \n",
    "    return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
